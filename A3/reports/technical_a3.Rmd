---
title: "SMEs Predictive Model"
author: "Aftab Alam & Khawaja Hassan"
date: "2/17/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,out.width="400px", dpi=120)
```

## Introduction

Investment managers need to allocate funds to opportunities where the return can be maximized. As consultants to an investment firm, the aim of our model was to predict probabilities of companies that would have a Compound Annual Growth Rate (CAGR) in sales of 40% or more between 2012 and 2014 and then classify the companies into two classes. This value of CAGR was used to classify fast-growth companies vs non-fast growth companies. Based on these classifications, the investment managers would then decide on whether to invest in the company or not. The value of this CAGR is driven because if a company has sales of 1 million in the year 2012, then what CAGR would result in a sale of around 1.5 million in the year 2014.The prediction analysis utilized the Logit, Logit LASSO, and Random Forest models with 5-fold cross validation. To arrive at the best model, we looked at RMSE, AUC, and the average expected loss as per our defined loss function. These models used several company features, such as balance sheet items, income statement items, and management related variables. 

## Data Selection

The data was prepared by Bisnode; however, it has been sourced from the [OSF ](https://osf.io/3qyut/) website. The dataset was relatively large and contained observations of more than 287,000 for more than 46,000 manufacturing and services’ firms with 48 explanatory variables in total. The time frame of the observations was from 2005 till 2016. Since the purpose of the project was to build a model that would be used to predict the fast-growing companies, we set the cut-off bar at 40% 2-years CAGR in sales of the companies. Companies having a CAGR of 40% or higher being classified as fast-growing companies and others being not-fast growing companies. Moreover, the time initially taken for data preparation was the panel data of these companies from 2010 till 2015. Based on this, we filtered on the companies that had data for these whole 6 years. 2-years CAGR was then calculated for 2012 till 2014. The reason behind choosing the 2012-2014 time-period instead of 2012-2013 was to ascertain the stable growth in sales as the sales number may fluctuate in a year-on-year change. Moreover, the reason for not choosing a longer time than 2012-2014 was the limitation in the prediction power as with longer time frames, prediction exercise becomes more and more difficult. 

Further, we created a dummy variable to identify the firms which are currently alive based on whether the sales were greater than 0 or if sales were NA. We only continued with the firms that were considered alive based on this criterion. Also, data was further narrowed down to only include small and medium sized companies. This was based on the number of sales being between 1000 Euros and 10 million Euros. Finally, for prediction, the year 2012 data was used with observations that had a CAGR of less than 3000. 

Moreover, after the prediction models were run on this filtered dataset, we went ahead and divided the dataset into two further subsets: 1- Manufacturing, 2- Services. The Manufacturing subset contained manufacturing companies only and the services dataset contained services companies only. The subsets were then divided into training and holdout samples to run the same analysis. The reason behind this was to compare the results of the main predictions with the individual subsets of manufacturing and services datasets.

## Data Engineering

We first looked at the distribution of sales, which was highly skewed, hence we created a new variable with log of sales that gave us a near-normal distribution. Then age of the firm was calculated by subtracting the founding year from the current year of the observation. Another industry category was column was created to club together several manufacturing and services’ industries, decreasing the number of levels compared to the original column. For financial columns, we created columns that contained normalized values for both balance sheet and income statement to make an apples-to-apples comparison across different sized companies. Balance sheet items were normalized by dividing the items from total assets. Total assets were calculated by combining intangible assets, current assets, and fixed assets. Similarly, income statement items were normalized by dividing them from total sales. A common theme across financial variables was the presence of negative values, which in the real world do not make much sense. Hence, these negative values were replaced with 0, for example in intangible assets, current assets, and fixed assets. However, before making these changes, flag variables were created to identify these values. The flags were then tested for variations in observations and the ones with no variations were dropped.Further, CEO age was calculated by subtracting birth-year from the current year of the observations. Flags were then created to identify CEO age; less than 25 being classified as low, greater than 75 as high, and missing for where the CEO age was coming out as NA. 

Missing values in labor average column were replaced by the mean value, this was saved as a new modified column. A flag variable was then created to identify the missing values. A new level variable was then created to classify fast growth companies and others where the value of 1 was assigned to fast-growth companies. After making all these modifications, we were then left with 10558 observations with 115 variables in total.Finally, to make it easier to use our variables in the prediction models, we stored specific variables into different groups based on our domain knowledge and data engineering we had conducted. These helped us in creating models with different levels of complexity. 

## Prediction Modeling

The datasets were divided into two subsets: training data (80%) and holdout data (20%). The training datasets were then used for 5-fold cross-validations in the models used ahead. In total, 5 probability logit models were run, 1 LASSO model, and 1 Random Forest model was run with provided tunning parameters. Details on each are as follows.
Probability Logit model

For probability logit models, we created 5 in total. Reason for not choosing simple OLS for predicting probabilities was because the probabilities may return with values of less than zero or higher than 1, which do not make sense in the real world. However, since the Logit model contains the predicted probabilities between zero and one, we decided to go with it. 
With regards to the 5 logit models, the complexity increased from 1 to 5, with model 1 being the simplest and model 5 being the most complex. The first model contained only 4 main predictors and the second model contained 10 predictors, both models containing variables on sales, profit and loss, and industry categories. However, for the later models, the complexity was increased significantly. 

Once the models were run, we looked at the AUC and the average RMSE of the 5-fold cross validation to determine the best model. The table below shows the average 5-fold cross-validated RMSE for the 5 logit models across the three datasets (Main: Combined observations of manufacturing and services companies, Manufacturing: Observations of manufacturing companies only, Services: Observations of services companies only). Based on these results, the best model amongst these was model number 2 for the main dataset with average 5-fold cross-validated RMSE of 0.3005829. The best model in the manufacturing dataset was model 1 with average 5-fold cross-validated RMSE of 0.3273329. Whereas the best logit model in services dataset was model 3 with average 5-fold cross-validated RMSE of 0.2829945.On the other hand, the highest AUC amongst these models in the main dataset was for model 4, which was only slightly higher than model 2, model 2 being the second best based on AUC. As shown below, the difference between RMSE values was not very significant, however, based on model complexity, AUC, and lowest RMSE, we decided to choose model 2 as the best amongst these. 

For manufacturing dataset, the highest AUC was for model 4 and AUC was highest for model 3 in the services dataset. However, as shown in the table below, using the same argument as used for the main dataset, we went ahead with choosing model 2 as best logit model for manufacturing and services dataset.

 <img src="crmse.png" width="220"/> <img src="mrmse.png" width="220"/> <img src="servicermse.png" width="220"/>


## Logit LASSO
For the second type of model, we ran a logit LASSO model with most complexity, we added the greatest number of variables in it. It included interactions, dummy variables, HR related variables, management related variables, and other important variables in the dataset. The final values used by the model were alpha equals 1 and lambda value around 0.00464. The average 5-fold cross-validated RMSE for this model was around 0.2996, which is lower than the best logit model mentioned above. However, the AUC for LASSO was much lower compared to the simple logit models, as shown in the table above.
With regards to the logit LASSO model run on manufacturing and services, the 5-fold cross-validated RMSE was higher than logit model number 2 in both dataset’s LASSO models. Like the logit LASSO average 5-fold cross validated AUC on the main dataset, it was lower or almost equal to logit model number 2 in both manufacturing and services datasets. 

## Probability Forest

To build a stronger prediction model, we decided to run a Random Forest (RF)/Probability Forest (PF) on the dataset. Although it is a black box, it is better at identifying non-linear relationships and interactions. Hence, we went ahead and used the predictors used in the model 4, but without any feature engineering. The model was run with tuning parameters, 5, 6, 7 number of random variables being used at splits and minimum node sizes of 10 and 15. In total, 500 trees were run. The model chose 7 as the number of random variables at splits and 15 as the minimum node size. As expected, the probability forest returned the lowest 5-fold cross-validated RMSE of around 0.2960 among all the models used so far and it also returned the highest AUC of around 0.7465, suggesting that this model being superior to all others when considered only under these two parameters.  This was the case for the RF models in both manufacturing and services datasets; it was superior in terms of both 5-fold cross-validated RMSE and AUC.

## ROC Curve

Based on diagnostics of the models, we chose tuned RF as our best model. Although, a backbox due to large number of decision trees, where each tree is trained on bagged data using random selection of features, the model is giving the best results. Since our investment management company is result oriented and is not bothered by how the model works, we decided to go ahead with this. This was the case across the three datasets.Once the best model was selected, we plotted a Receiver Operating Characteristic (ROC) curve for the models across the datasets using discrete thresholds between 0.05 and 0.75. The below ROC curve is for the RF model ran on the main dataset. All three curves resulted in a curve with decreasing slope, however, the curves remained above the 45 degree line, suggesting the predictions made by the RF model were better than a prediction obtained from a fair coin toss.

<img src="cdot.png" width="400"/> <img src="c-AUC.png" width="400"/>


## Loss Function
We defined our loss function keeping in mind two things, the risk-free interest rate paid by depositing the money in a bank and the rate of return on investing money in a company. We looked at the current interest rate provided by Hungarian banks on deposits as the risk-free rate, 3.3%. Whereas we assumed the rate of return on investment in a fast-growing company would be 10%. Another assumption while creating the loss function was that if an investment manager invests in a company that turns out to be non-fast-growing company, the investment manager would get 0% return out of the investment.
Based on these assumptions, we calculated the opportunity costs to arrive at the relative losses by false negatives and false positives. If the manager invests in a company and the classification was false positive, then the manager would lose out on the 3.3% return that could have been earned from depositing the money in a bank. Hence the cost of a false positive is 3.3% risk free return. 

On the other hand, if the investment manager does not invest in the company based on a false negative classification, the loss would be 6.7% as the money would be deposited in the bank and the money will still earn 3.3% (10% - 3.3%). Based on this, the ratio of cost of FP and FN would be 1:2, where false negative being twice as costly as the false positive cost.
Optimal Threshold & Classification
The optimal classification threshold based on these relative costs was calculated as 0.33. This was calculated using the optimal classification threshold formula that assumes that the model being used is the best one for prediction, which may not be true in practicality. 

Hence, we carried out the exercise of calculating the optimal threshold using the data itself while incorporating our loss function. We plotted ROC curves to find the optimum threshold. For the main dataset, optimum threshold was 0.35, whereas it 0.27 for the manufacturing dataset and 0.25 for the services dataset. 
Based on these classifications, in the main dataset, any predicted probability of 0.35 or above would be classified as fast-growth, 0.27 or above as fast-growth in the manufacturing dataset, and 0.25 or above as fast-growth in the services dataset. 

The plots below show the AUC based on our defined loss function. 

<img src="manuAUC.png" width="400"/> <img src="serviceAUC.png" width="400"/>


## Confusion Matrices

The first confusion matrix was built without the loss function, which runs on the majority vote ideology, where it assigns the value of 1 to any predicted probability of 0.5 or above. This is not the optimum threshold as the losses from false positive and false negative are not always symmetric in the real world. Given that false negatives are more costly in our case, the goal would be to reduce the occurrence of false negatives in our predictions. However, if we first look at the 0.5 threshold matrix, the percentage of false negatives is around 10.1% and percentage of false positives is 0.56%, whereas, with a 0.35 threshold, the percentage of false negatives is 9.1% and percentage of false positives is 3.12%.Based on our loss function, the main model suggests that the company loses out around 1,176 Euros per firm and if the company evaluates 1000 firms in a year, the company loses out around 1.176 million Euros. However, when focusing on the confusion matrices for individual manufacturing dataset and services dataset, the results are starkly different.

<img src="combined_cf.png" width="600"/>

The manufacturing dataset predictions suggest that using our predictions saves the company around 590 Euros compared to the 0.5 threshold and if the company evaluates 1000 firms in a year, then the company saves 589,743 Euros in total.

<img src="manu_cf.png" width="600"/>

Similarly, services dataset prediction classification suggests that our investment firm will save 1090 Euros per firm and if the company evaluates 1000 firms per year, the company will save around 1.1 million Euros per year. 

<img src="service_cf.png" width="600"/>

## Conclusion
Based on our prediction models, the best model has been the random forest model across all the defined datasets. Although a black box, random forest is result oriented and is optimum for our investment management company, which is also result oriented. Moreover, our analysis suggests that training the best model on specific industry dataset is better when making classification models compared to training the model on one larger dataset that contains multiple industries. This makes sense in the real world as well where models are trained on same types of companies to make better predictions.However, to check for external validity, we highly recommend running these models on a different time periods than 2012-2014 time period, perhaps checking multiple periods would be wise, such 2013-2015, 2014-2016 etc. Additional to this, we would suggest collecting more observations on industry specific small and medium sized firms separately and running this prediction exercise. Perhaps that would give focused results, helping the investment company further.

[Link to RMD codes - GITHUB](https://github.com/Aftab1995/DA3/tree/main/A3)
